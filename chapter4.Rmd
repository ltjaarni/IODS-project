# A case study in linear discriminant analysis and K-means clustering

_In the following, we will use linear discriminant analysis and K-means clustering for grouping our data on housing in the Boston area. The dataset we utilize is openly available from the MASS-package in R._

We have a somewhat similar task at our hands as with logistic regression: our purpose is to be able to sort our dataset into distinct classes. One way to do this is to predetermine the class labels, and then utilize the features in the dataset to predict those labels. This classification task is then a form of supervised learning, one example of which we already introduced in the above with logistic regression. Another alternative for completing a similar task is that of linear discriminant analysis, a limitation of which is that the predictors ought to be normally distributed (which also implies they must be continuous, of course). 

Another approach to grouping data items is that inherent in unsupervised learning, where the correct class labels are not provided in the training dataset, but are instead determined by the algorithm itself, usually based on some distance measure between the groups induced. A classical example is that of K-means clustering where one must provide the number of groups for the algorithms, even if not their labels. We will look into both approaches in what follows.

```{r, echo=FALSE}
date()
```

We read in the openly available dataset ''Boston'' from the MASS-package and take our first look at it.

```{r, warning=FALSE}

# We require the MASS package for our dataset
library(MASS)

# load the data
data("Boston")

# explore the dataset
str(Boston)

```

We have a dataset with 506 observations on 14 variables. We have 12 numeric variables and 2 integer-valued variables. Apparently the statistical units here are suburbs or towns within the Boston area. This dataset has no missing values.

```{r, warning=FALSE}

sum(is.na(Boston))

```
''Crim'' is a variable on the crime rate per capita; ''zn'' is proportion of residential land zoned; ''indus'' is proportion of non-retail business acres per town; ''chas'' is an indicator for the suburb being adjacent to Charles river; ''nox'' is nitrix oxides concentration in air; ''rm'' is average rooms per apartment; ''age'' is proportion of owner-occupied units built prior to 1940; ''dis'' is a distance measure to five Boston employment centres; ''rad'' measures highway accessibility; ''tax'' measures property tax rate; ''ptratio'' is the pupil-teacher ratio; ''black'' measures the proportion of african-americans, ''lstat'' proportion that is lower status of the population (whatever that might mean!) and ''medv'' median value of homes in thousands of dollars.

The variables measure a wide range of interesting characteristics of the Boston area suburbs. This makes for a dataset that is rich in content, even if not very large in terms of its dimensions.

```{r, warning=FALSE}
summary(Boston)
```
Let us plot these as kernel densities.

```{r, warning=FALSE}
# draw a bar plot of each variable

# we require dplyr, tidyr and ggplot for the below code chunk compiling the kernely densities
library(dplyr)
library(tidyr)
library(ggplot2)

gather(Boston) %>% ggplot(aes(value)) + geom_density() + facet_wrap("key", scales = "free")
```

We see that there is quite a lot of variability in terms of the suburbs in the Boston area. There are large differences in terms of the crime rate, the median value of the the houses, pollution, average count of rooms per apartment and even in the pupil to teacher ratio. By and large this looks quite promising in terms of ''profiling'' the areas into distinct categories.

Yet we also observe that the variables are anything but normal, which is an issue for linear discriminant analysis.

Since we have a dataframe of mostly numeric and interval-valued variables, let us take a look at the pairwise Pearson correlation coefficients between the variables to see if some associations stand out.

```{r, warning=FALSE}

# We require the following libraries for the correlation plot
library(tidyr)
library(corrplot)

# calculate the correlation matrix and round it
cor_matrix <- Boston %>% cor() %>% round(digits=2)

# print the correlation matrix
cor_matrix

```

We have quite high Pearson correlations between the variables. This seems in a sense intuitive, when looking at the variables measured, and their distributions, which indicate that the suburbs in the Boston area are quite heterogeneous. 

While suburbs are different from each other, they tend to become, through time, internally more and more homogeneous. What we often get is a mechanism of ''preferential attachment'', where alike attract alike, along a geographical accumulation of challenges, resulting in issues such as segregation, and polarization between the ''good'' and the ''bad'' neighbourhoods.

Let us look at a plot of the correlation matrix to see what pairwise profiles pop out.

```{r, warning=FALSE}
# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex=0.6)

```

When looking at individual variables, we can see that ''indus'' and ''nox'' have particularly high absolute pairwise correlations with the rest of the variables, which is quite interesting, given the range of the content of the variables from educational issues to prizing and crime. ''Indus'' and ''nox'' have a very high correlation, which is unsurprising, since the former measures non-retail commercial ares (which I guess is mostly industry) and ''nox'' pollution. We see that industrial areas are ones that are close to the highways, have high property tax rate and relatively high crime rates. The ''polluted'' areas have similar profiles, since the correlation between industry and pollution is so high.

If we restrict our attention to the first row of the correlation matrix, since we are soon to delve more closely into the issue of crime, we may observe that the highest absolute correlations are with highway accessibility, industry, air pollution, proportion of ''low status'' population and property tax rate. All these have a Pearson sample correlation coefficient of above 0.4.

Let us standardize the data: subtract by the mean and divide by the standard deviation.  

```{r, warning=FALSE}
# center and standardize variables
boston_scaled <- Boston %>% scale() %>% as.data.frame()
                          
# summaries of the scaled variables
summary(boston_scaled)

```
Each of our variables now have a mean of zero and a standard deviation of 1, but the shapes of the kernels remain as before: clearly nonnormal.

```{r, warning=FALSE}
# draw a bar plot of each variable
gather(boston_scaled) %>% ggplot(aes(value)) + geom_density() + facet_wrap("key", scales = "free")
```

We next turn the crime rate into a factor by utilizing the quartiles as the cut-off point for each level of the factor. We also remove the old crime variable and insert the novel factor into our dataframe.

```{r}

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels=c("low", "med_low", "med_high", "high"))

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

# look at the table of the new factor crime
table(crime)

```

Having used quartiles as the cut-off points, we have groups of the same size, which will be handy for linear discriminant analysis. The names for the levels are in an ascending order of the crime rate.

We then divide our dataframe into a training and a test dataset, so that a random 80% of the rows go into the first and the rest to the second dataset.

```{r, warning=FALSE}

# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

set.seed(13)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

```

_Even though our predictors our anything but normally distributed_, We next fit linear discrimant analysis to the dataset, using the crime factor variable as the predicted class and the rest of the dataset as predictors to the class.

```{r, warning=FALSE}

set.seed(13)

# linear discriminant analysis
lda.fit <- lda(crime~., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  graphics::arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results (select both lines and execute them at the same time!)
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)

```

We may evaluate the fit by looking at how well our model predicts the classes of the test dataset on which it has not been fitted. Our test set has 102 observations. 

```{r, warning=FALSE}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)

```

We have seem to have quite good classification performance: with the correct predictions found on the diagonal, we have a sum total of 70, which comes down to an accuracy of $70/102\sim=0.686$.With a balanced baseline, so that all classes are of the same size, withour our predictors and our model, we would have an accuracy of $0.25$. Compared to that our model classifies extremely well.

We get the ''high'' class correctly predicted especially well, with all observations of the test set belonging to that category being classified correctly. It proves more difficult to us to make the distinctions for the two low-groups than the two high groups: when the correct class is low, we get our prediction correct with proportion $16/26=0.615$, with $7$ observations predicted to the med_low category; when the correct class is med_low, we get our prediction correct $12/26=0.462$ of the time, with 14 incorrectly predicted in the low or the med_high category.

This means that we would be better able to classify observations into fewer classes: or, to put it differently, it seems there is relatively little in our features that would allow us to distinguish between the suburbs with low and med_low crime rates, whereas we are better at distinguishing between the two low categories and the two high categories. We are especially apt at identifying the high crime rate suburbs. These suburbs are apparently characterized by features that they very much share with each other but which the other classes of suburbs do not have.  