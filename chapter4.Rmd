# A case study in linear discriminant analysis and K-means clustering

_In the following, we will use linear discriminant analysis and K-means clustering for grouping our data on housing in the Boston area. The dataset we utilize is openly available from the MASS-package in R._

We have a somewhat similar task at our hands as with logistic regression: our purpose is to be able to sort our dataset into distinct classes. One way to do this is to predetermine the class labels, and then utilize the features in the dataset to predict those labels. This classification task is then a form of supervised learning, one example of which we already introduced in the above with logistic regression. Another alternative for completing a similar task is that of linear discriminant analysis, a limitation of which is that the predictors ought to be normally distributed (which also implies they must be continuous, of course). 

Another approach to grouping data items is that inherent in unsupervised learning, where the correct class labels are not provided in the training dataset, but are instead determined by the algorithm itself, usually based on some distance measure between the groups induced. A classical example is that of K-means clustering where one must provide the number of groups for the algorithms, even if not their labels. We will look into both approaches in what follows.

```{r, echo=FALSE}
date()
```
## Load and describe the dataset

We read in the openly available dataset ''Boston'' from the MASS-package and take our first look at it.

```{r, warning=FALSE}

# We require the MASS package for our dataset
library(MASS)

# load the data
data("Boston")

# explore the dataset
str(Boston)

```

We have a dataset with 506 observations on 14 variables. We have 12 numeric variables and 2 integer-valued variables. Apparently the statistical units here are suburbs or towns within the Boston area. This dataset has no missing values.

```{r, warning=FALSE}

sum(is.na(Boston))

```
''Crim'' is a variable on the crime rate per capita; ''zn'' is proportion of residential land zoned; ''indus'' is proportion of non-retail business acres per town; ''chas'' is an indicator for the suburb being adjacent to Charles river; ''nox'' is nitrix oxides concentration in air; ''rm'' is average rooms per apartment; ''age'' is proportion of owner-occupied units built prior to 1940; ''dis'' is a distance measure to five Boston employment centres; ''rad'' measures highway accessibility; ''tax'' measures property tax rate; ''ptratio'' is the pupil-teacher ratio; ''black'' measures the proportion of african-americans, ''lstat'' proportion that is lower status of the population (whatever that might mean!) and ''medv'' median value of homes in thousands of dollars.


The variables measure a wide range of interesting characteristics of the Boston area suburbs. This makes for a dataset that is rich in content, even if not very large in terms of its dimensions.

## Summary and graphical overview of the variables in the dataset

Let us take a quick look at summaries of the uni-dimensional distributions of the variables and their kernel densities.

```{r, warning=FALSE}
summary(Boston)
```
Let us plot these as kernel densities.

```{r, warning=FALSE}
# draw a bar plot of each variable

# we require dplyr, tidyr and ggplot for the below code chunk compiling the kernely densities
library(dplyr)
library(tidyr)
library(ggplot2)

gather(Boston) %>% ggplot(aes(value)) + geom_density() + facet_wrap("key", scales = "free")
```

We see that there is quite a lot of variability in terms of the suburbs in the Boston area. There are large differences in terms of the crime rate, the median value of the the houses, pollution, average count of rooms per apartment and even in the pupil to teacher ratio. By and large this looks quite promising in terms of ''profiling'' the areas into distinct categories.

Yet we also observe that the variables are anything but normal, which is an issue for linear discriminant analysis.

Since we have a dataframe of mostly numeric and interval-valued variables, let us take a look at the pairwise Pearson correlation coefficients between the variables to see if some associations stand out.

```{r, warning=FALSE}

# We require the following libraries for the correlation plot
library(tidyr)
library(corrplot)

# calculate the correlation matrix and round it
cor_matrix <- Boston %>% cor() %>% round(digits=2)

# print the correlation matrix
cor_matrix

```

We have quite high Pearson correlations between the variables. This seems in a sense intuitive, when looking at the variables measured, and their unidimensional distributions, which indicate that the suburbs in the Boston area are quite heterogeneous.
High unidimensional variance makes room for meaningful correlation structures; the fact that one may posit that some values of features tend to go together, just by looking at what they measure (a simple example, say, pollution and industry), has one believe that such sructures will indeed emerge from our data. Thus, even if suburbs or groups thereof started out as relatively different from each other, they or their groups would tend to become, through time, internally more and more homogeneous. For what we often get is a mechanism of ''preferential attachment'', where alike attract alike, along a geographical accumulation of challenges, resulting in issues such as segregation, and polarization between the ''good'' and the ''bad'' neighbourhoods. 

To provide an illustrative if crass example: if one neighbourhood had, initially, somewhat higher proportion of rich citizens, as a further such rich citizen contemplates moving from one suburb to another, or entirely from outside of the area and into it, we would more likely choose the areas with a relatively high proportion of the rich, and the probability of the choice of the suburb might be directly proportional with the proportion of the wealthy. Then, as the process iterates, the proportion of the wealthy grows larger, and along it the probability of the wealthy moving in. At some point the prizes of the region would climb to a level, that the the rich would start to favor the area with the second highest proportion of the rich, and or move to a suburb adjacent to the wealthy neighbourhood. Thus, even if one had, at first, in absolute standards, relatively homogeneous suburbs by wealth, one would tend to get an increased variance of wealth across the suburbs through time, with clear clusters to the groups by wealth. Of course, wealth would bring along with it other distinctive features to the neighbourhoo, resulting in a formation of multivariate clusters.

Let us look at a plot of the correlation matrix to see what pairwise profiles of suburbs pop out.

```{r, warning=FALSE}
# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex=0.6)

```

When looking at individual variables, we can see that ''indus'' and ''nox'' have particularly high absolute pairwise correlations with the rest of the variables, which is quite interesting, given the range of the content of the variables from educational issues to prizing and crime. ''Indus'' and ''nox'' have a very high correlation, which is unsurprising, since the former measures non-retail commercial ares (which I guess is mostly industry) and ''nox'' pollution. We see that industrial areas are ones that are close to the highways, have high property tax rate and relatively high crime rates. The ''polluted'' areas have similar profiles, since the correlation between industry and pollution is so high.

If we restrict our attention to the first row of the correlation matrix, since we are soon to delve more closely into the issue of crime, we may observe that the highest absolute correlations are with highway accessibility, industry, air pollution, proportion of ''low status'' population and property tax rate. All these have a Pearson sample correlation coefficient of above 0.4.

## Standardization and other preprocessing prior to analysis

Let us standardize the data: subtract by the mean and divide by the standard deviation. This might be useful given the different variances and ranges to the variables. If we did not scale, large numerical distances would be found on the basis of those variables with high variance and range, even though such induced differences might not be large in the conceptual sense. On the contrary, when we are comparing observations on the standardized scale, we are interpreting ''large'' in the appropriate context of the variable that accounts for the fact that different variables are measured on very different scales (even if they were all interval-scaled). A seemingly large difference of, say, 4, between observations in the original scale of a variable with a variance of 25, ought by default not be considered as large as a difference of 2 in terms of a variable with variance of 1. In short, without scaling, identified differences in terms of distances would tend to be driven by the absolute differences in the observations on those variables with high variance, even if such differences were not conceptually large or even noteworthy.       

```{r, warning=FALSE}
# center and standardize variables
boston_scaled <- Boston %>% scale() %>% as.data.frame()
                          
# summaries of the scaled variables
summary(boston_scaled)

```
Each of our variables now have a mean of zero and a standard deviation of 1. It is important to emphasize that the shapes to the kernels remain as before: clearly nonnormal.

```{r, warning=FALSE}
# draw a bar plot of each variable
gather(boston_scaled) %>% ggplot(aes(value)) + geom_density() + facet_wrap("key", scales = "free")
```

We next turn the crime rate into a factor by utilizing the quartiles as the cut-off points for each level of the factor. We also remove the old crime variable and insert the novel factor into our dataframe.

```{r}

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels=c("low", "med_low", "med_high", "high"))

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

# look at the table of the new factor crime
table(crime)

```

Having used quartiles as the cut-off points, we have groups of the same size, which will be handy for linear discriminant analysis. The names for the levels are in an ascending order of the crime rate.

We then divide our dataframe into a training and a test dataset, so that a random 80% of the rows go into the first and the rest to the second dataset.

```{r, warning=FALSE}

# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

set.seed(13)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

```

## Identifying the level of the crime factor with linear discriminant analysis

_Even though our predictors our anything but normally distributed_, we next fit linear discrimant analysis to the dataset, using the crime factor variable as the predicted class and the rest of the dataset as predictors to the class.

```{r, warning=FALSE}

set.seed(13)

# linear discriminant analysis
lda.fit <- lda(crime~., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  graphics::arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results (select both lines and execute them at the same time!)
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)

```
It seems our first linear discriminant allows us to identify all ''high class'' suburbs into their own group, bringing some ''med high'' suburbs along with them. Also our second linear disciminant is able to differentiate the groups, but there is much more considerable overlap here. It seems that for our rather clearly separate cluster with high crimes rates formed around $LD1\sim6$, $LD2\sim0$, the ''med high'' group tends to get somewhat higher values on $LD2$. For our other clearly separate cluster with low to medium high crime rates, located in the neighbourhood of $LD1\sim-2$, $LD2\sim-0.5$, our ''low class'' gets somewhat higher values on LD1 and lower values on LD2 that med low and med high; similar observation applies for med low in comparison to med high. But, as mentioned, this cluster of ''low to med high'' crime rates does not have clearly distinct but somewhat overlapping, even if distinguishable subclusters. 

## Predicting the level of the crime factor with linear discriminant analysis

So it would seem on the basis of the visual plot that our linear discriminant analysis holds promise in terms of predicting crime factor levels. We may evaluate the fit more closely by looking at how well our model predicts the classes of the test dataset on which it has not been fitted. Our test set has 102 observations. 

```{r, warning=FALSE}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)

```

We have seem to have quite good classification performance: with the correct predictions found on the diagonal, we have a sum total of 64, which comes down to an accuracy of $64/102\sim=0.628$.With a balanced baseline, so that all classes are of the same size, without our predictors and our model, we would have an accuracy of $0.25$. Compared to that our model classifies relatively well.

We get the ''high'' class correctly predicted especially well, with all observations of the test set belonging to that category being classified correctly. It proves more difficult for us to make the distinctions for the two low-groups than the two high groups (as we detected already from our visual): when the correct class is low, we get our prediction correct with proportion $10/26=0.385$, with $12$ observations predicted to the med_low category; when the correct class is med_low, we get our prediction correct $16/28=0.571$ of the time, with 12 incorrectly predicted in the low or the med_high category.

This means that we would be better able to classify observations into fewer classes: or, to put it differently, it seems there is relatively little in our features that would allow us to distinguish between the suburbs with low and med_low crime rates, whereas we are better at distinguishing between the two low categories and the two high categories. We are especially apt at identifying the high crime rate suburbs. These suburbs are apparently characterized by features that they very much share with each other but which the other classes of suburbs do not have.

## K-means clustering

We next move on to our second topic and K-means clustering. We first look at some distance measures one could perhaps utilize as the basis of the creation of the clusters. To this end, we reload the Boston dataset, scale it, and calculate the Euclidean distance and the Manhattan distance on it. We then summarize the distances between the rows of our dataset with the generic summary-function (each row of our dataset is just a vector of numbers, so that a distance calculation makes sense, as does a summary of the distances between the rows).

```{r, warning=FALSE}
# Rescale Boston dataset and retain its type as a data.frame object
Boston_scaled2<-as.data.frame(scale(Boston))

# Euclidean distance matrix
dist_eu <- dist(Boston_scaled2)

# look at the summary of the distances
summary(dist_eu)

# manhattan distance matrix
dist_man <- dist(Boston, method="manhattan")

# look at the summary of the distances
summary(dist_man)

```

We then run the K-means clustering algorithm, first with four clusters. We print the clusters as functions of the predictors. Since there are 14 predictors, we split the visuals into three partitions of the variables.

```{r, warning=FALSE}

# Set seed for reproducibility
set.seed(123)

# k-means clustering
km <- kmeans(Boston_scaled2, centers = 4)

# plot the Boston dataset with clusters, variables 1 to 5
pairs(Boston_scaled2[1:5], col = km$cluster)

# plot the Boston dataset with clusters, variables 6 to 10
pairs(Boston_scaled2[6:10], col = km$cluster)

# plot the Boston dataset with clusters, variables 10 to 15
pairs(Boston_scaled2[11:14], col = km$cluster)

```

We note from the two-dimensional visuals of the clusters that while we have an ability to distinguish the clusters, these tend to overlap, with the possible exception of the cluster colored in black. This cluster can be distinguished from the others pretty well in terms of a low value measured on the variable ''black''.  

We then aim to determine an appropriate number of clusters in a data-driven way. We set an upper bound (10) on the number of clusters that our algorithm infers from our data, and then see what count of clusters seems best aligned with our dataset.

```{r, warning=FALSE}
# Set seed for reproducibility
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston_scaled2, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

```
The logic of the plot is based on an idea of identifying where, in terms of the count of clusters, we have a relatively steep descent in the within-cluster sum of squares (which we may consider the loss we'd like to minimize). This sum of squares always declines with the count of clusters, so one cannot simply minimize the within-cluster sum of squares without constraints (making each group as internally homogeneous as possible) -- for this would always imply maximizing the count of clusters. Instead, one looks for a point where we have _the most added value from an additional cluster_: and this is exactly where we have, to the left of the count of the clusters on the x-axis, a steep descent in the value on the y-axis (within-cluster sum of sqaures) of the graph in the above. We may observe in our graph that the descent starts to level off at roughly two to three clusters (slowly tending towards a horizontal line representing a constant wcss), while moving from one to two clusters implies a steep decline in within-cluster sum of squares. This means that two clusters would seem appropriate on the basis of our data. Here, it seems we have relative difficulties in distinguishing between the observations if we aim to compile more clusters than the two (because the groups are already quite internally homogenenous with two clusters). So we settle for the two, fit the model and visualise it with a similar pairs-visual as in the above.

```{r, warning=FALSE}
#set seed for reproducibility
set.seed(123)

# k-means clustering
km <- kmeans(Boston_scaled2, centers = 2)

# plot the Boston dataset with clusters, variables 1 to 5
pairs(Boston_scaled2[1:5], col = km$cluster)

# plot the Boston dataset with clusters, variables 6 to 10
pairs(Boston_scaled2[6:10], col = km$cluster)

# plot the Boston dataset with clusters, variables 10 to 15
pairs(Boston_scaled2[11:14], col = km$cluster)
```

We may note from the plots that we have no difficulties in distinguishing the two clusters from each other: already in the two-dimensional scatter plots we may observe quite clear separation of the group clusters. Here the separation is more evident as before in terms of other pairwise plots than just those including the variable ''black''. 